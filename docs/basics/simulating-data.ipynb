{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "In this notebook, we're going to get some practice writing data generating processes,\n",
    "and calculating joint likelihoods between our data and model,\n",
    "using the SciPy statistics library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating coin flips (again!)\n",
    "\n",
    "We're going to stick with coin flip simulations, because it's:\n",
    "\n",
    "1. incredibly simple,\n",
    "2. incredibly informative,\n",
    "3. incredibly extensible.\n",
    "\n",
    "This time, though, we're going to construct a model of coin flips\n",
    "that no longer involves a fixed/known $p$,\n",
    "but instead involves a $p$ that is not precisely known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithmic protocol\n",
    "\n",
    "If we have a $p$ that is not precisely known, we can set it up by instantiating a probability distribution for it, rather than a fixed value.\n",
    "\n",
    "How do we decide what distribution to use?\n",
    "Primarily, the criteria that should guide us is the _support_ of the distribution,\n",
    "that is, the range of values for which the probability distribution is valid.\n",
    "\n",
    "$p$ must be a value that is bounded between 0 and 1.\n",
    "As such, the choice of probability distribution for $p$ is most intuitively the Beta distribution,\n",
    "which provides a probability distribution over the interval $[0, 1]$.\n",
    "\n",
    "Taking that value drawn from the Beta, we can pass it into the Bernoulli distribution,\n",
    "and then draw an outcome (either 1 or 0).\n",
    "In doing so, we now have the makings of a __generative model__ for our coin flip data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating in code\n",
    "\n",
    "Let's see the algorithmic protocol above implemented in code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats as sts\n",
    "import numpy as np\n",
    "\n",
    "def coin_flip_generator() -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Coin flip generator for a `p` that is not precisely known.\n",
    "    \"\"\"\n",
    "    p = sts.beta(a=10, b=10).rvs(1)\n",
    "    result = sts.bernoulli(p=p).rvs(1)\n",
    "    return result\n",
    "\n",
    "coin_flip_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Information\n",
    "\n",
    "The astute eyes amongst you will notice \n",
    "that the Beta distribution has parameters of its own,\n",
    "so how do we instantiate that?\n",
    "Well, one thing we can do is bring in some _prior information_ to the problem.\n",
    "\n",
    "Is our mental model of this coin that it behaves like billions of other coins in circulation,\n",
    "in that it will generate outcomes with basically equal probability?\n",
    "Turns out, the Beta distribution can assign credibility in this highly opinionated fashion!\n",
    "And by doing so, we are injecting _prior information_\n",
    "by instantiating a Beta _prior distribution_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import FloatSlider, interact, Checkbox\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "alpha = FloatSlider(value=2, min=1.0, max=100, step=1, description=r'$\\alpha$')\n",
    "beta = FloatSlider(value=2, min=1.0, max=100, step=1, description=r'$\\beta$')\n",
    "equal = Checkbox(value=False, description=r\"set $\\beta$ to be equal to $\\alpha$\")\n",
    "\n",
    "@interact(alpha=alpha, beta=beta, equal=equal)\n",
    "def visualize_beta_distribution(alpha, beta, equal):\n",
    "    if equal:\n",
    "        beta = alpha\n",
    "    dist = sts.beta(a=alpha, b=beta)\n",
    "    xs = np.linspace(0, 1, 100)\n",
    "    ys = dist.pdf(xs)\n",
    "    plt.plot(xs, ys)\n",
    "    plt.title(fr\"$\\alpha$={alpha}, $\\beta$={beta}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you play around with the slider, notice how when you increase the $\\alpha$ and $\\beta$ sliders,\n",
    "the width of the probability distribution decreases,\n",
    "while the height of the maximum value increases,\n",
    "thus reflecting greater _certianty_ in what values for $p$ get drawn.\n",
    "Using this _prior distribution_ on $p$, we can express what we think is reasonable\n",
    "given _prior knowledge_ of our system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justifying priors\n",
    "\n",
    "Some of you, at this point, might be wondering - is there an algorithmic protocol for justifying our priors too?\n",
    "Can we somehow \"just pass our priors into a machine and have it tell us if we're right or wrong\"?\n",
    "\n",
    "It's a great wish, but remains just that: wishful thinking.\n",
    "Just like the \"Aye Eye Drug\", one for which a disease is plugged in,\n",
    "and the target and molecule are spat out.\n",
    "(I also find it to not be an inspiring goal,\n",
    "as the fun of discovery is removed.)\n",
    "\n",
    "Rather, as with all modelling exercises,\n",
    "I advocate for human debate about the model.\n",
    "After all, humans are the ones taking action based on, and being affected by, the modelling exercise.\n",
    "There are a few questions we can ask to help us decide:\n",
    "\n",
    "- Are the prior assumptions something a _reasonable_ person would make?\n",
    "- Is there evidence that lie outside of our problem that can help us justify these priors?\n",
    "- Is there a _practical_ difference between two different priors?\n",
    "- In the limit of infinite data, do various priors converge? (We will see later how this convergence can happen.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "It's time for some exercises.\n",
    "\n",
    "### Exercise: Control prior distribution\n",
    "\n",
    "In this first exercise, I would like you to modify the `coin_flip_generator` function\n",
    "such that it allows a user to control what the prior distribution on $p$ should look like\n",
    "before returning outcomes drawn from the Bernoulli.\n",
    "\n",
    "Be sure to check that the values of `alpha` and `beta` are valid values, i.e. floats greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coin_flip_generator_v2(alpha: float, beta: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Coin flip generator for a `p` that is not precisely known.\n",
    "    \"\"\"\n",
    "    if alpha < 0:\n",
    "        raise ValueError(f\"alpha must be positive, but you passed in {alpha}\")\n",
    "    if beta < 0:\n",
    "        raise ValueError(f\"beta must be positive, but you passed in {beta}.\")\n",
    "    p = sts.beta(a=alpha, b=beta).rvs(1)\n",
    "    result = sts.bernoulli(p=p).rvs(1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Simulate data\n",
    "\n",
    "Now, simulate data generated from your new coin flip generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def generate_data(n_draws: int, alpha: float, beta: float) -> List[int]:\n",
    "    \"\"\"\n",
    "    Generate n draws from the coin flip generator.\n",
    "    \"\"\"\n",
    "    data = [coin_flip_generator_v2(alpha, beta) for _ in range(n_draws)]\n",
    "    return np.array(data).flatten()\n",
    "\n",
    "generate_data(50, alpha=5, beta=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that written, we now have a \"data generating\" function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint likelihood\n",
    "\n",
    "Remember back in the first notebook how we wrote about evaluating the joint likelihood of multiple coin flip data\n",
    "against an assumed Bernoulli model?\n",
    "\n",
    "We wrote a function that looked something like the following:\n",
    "\n",
    "```python\n",
    "from scipy import stats as sts\n",
    "from typing import List\n",
    "\n",
    "def likelihood(data: List[int]):\n",
    "    c = sts.bernoulli(p=0.5)\n",
    "    return np.product(c.pmf(data))\n",
    "```\n",
    "\n",
    "Now, if $p$ is something that is not precisely known,\n",
    "then any \"guesses\" of $p$ will have to be subject to the Likelihood principle too,\n",
    "which means that we need to jointly evaluate the likelihood of $p$ and our data.\n",
    "\n",
    "Let's see that in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_likelihood(data: List[int], p: float) -> float:\n",
    "    p_like = sts.beta(a=10, b=10).pdf(p)  # our priors are stated here\n",
    "    data_like = sts.bernoulli(p=p).pmf(data)\n",
    "    \n",
    "    return np.product(data_like) * np.product(p_like)\n",
    "\n",
    "joint_likelihood([1, 1, 0, 1], 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint _log_-likelihood\n",
    "\n",
    "Because we are dealing with decimal numbers,\n",
    "when multiplying them together,\n",
    "we might end up with underflow issues.\n",
    "As such, we often take the log of the likelihood.\n",
    "\n",
    "### Exercise: Implementing joint _log_-likelihood\n",
    "\n",
    "Doing this means we can use summations on our likelihood calculations,\n",
    "rather than products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_loglike(data: List[int], p: float) -> float:\n",
    "    p_loglike = sts.beta(a=10, b=10).logpdf(p)  # our priors are stated here\n",
    "    data_loglike = sts.bernoulli(p=p).logpmf(data)\n",
    "    \n",
    "    return np.sum(data_loglike) + np.sum(p_loglike)\n",
    "joint_loglike([1, 1, 0, 1], 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Confirm equality\n",
    "\n",
    "Now confirm that the joint log-likelihood is of the same value as the log of the joint likelihood,\n",
    "subject to machine precision error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(joint_likelihood([1, 1, 0, 1], 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian-modelling-tutorial",
   "language": "python",
   "name": "bayesian-modelling-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
